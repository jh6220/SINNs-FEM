{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "import matplotlib.path as mpath\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tensorflow as tf\n",
    "import calfem.geometry as cfg\n",
    "import calfem.mesh as cfm\n",
    "import calfem.vis as cfv\n",
    "from scipy.interpolate import LinearNDInterpolator, CloughTocher2DInterpolator, CubicSpline, interp1d, PchipInterpolator, RegularGridInterpolator\n",
    "from scipy.spatial import Delaunay\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Reshape, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_am_file(file_path):    \n",
    "    # Search for the binary data section marker \"@1\"\n",
    "    marker = b\"@1\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        content = file.read()\n",
    "        start = content.find(marker) + len(marker)\n",
    "        \n",
    "        # Assuming there are two newline characters after the \"@1\" marker\n",
    "        # Adjust if necessary based on the actual file format\n",
    "        start += 32\n",
    "        \n",
    "        # The total number of data points is the product of the lattice dimensions\n",
    "        # Each point has 2 float components\n",
    "        num_points = 512 * 512 * 1001 * 2\n",
    "        \n",
    "        # Set the file pointer to the start of the binary data and read it\n",
    "        file.seek(start)\n",
    "        data = np.fromfile(file, dtype=np.float32)\n",
    "        \n",
    "        # Reshape the data to the correct dimensions (512, 512, 1001, 2)\n",
    "        # The last dimension is 2 for the two components of velocity at each grid point\n",
    "        data = data.reshape((1001,512, 512, 2))\n",
    "        return data\n",
    "    \n",
    "class IsInDomain:\n",
    "    def __init__(self, nodesCurves):\n",
    "        self.loops = [mpath.Path(nodesCurves[i]) for i in range(len(nodesCurves))]\n",
    "        \n",
    "    def __call__(self, points):\n",
    "        return np.logical_and(self.loops[0].contains_points(points),np.logical_not(np.array([loop.contains_points(points) for loop in self.loops[1:]]).any(0)))\n",
    "    \n",
    "class Interp2dAcrossTimesteps:\n",
    "    def __init__(self, data, x_coords, y_coords, kind='linear'):\n",
    "        # self.data = data\n",
    "        # self.x_coords = x_coords\n",
    "        # self.y_coords = y_coords\n",
    "        self.interp = [RegularGridInterpolator((x_coords,y_coords),  np.transpose(data[i,:,:,:],(1,0,2)), method=kind, bounds_error=False, fill_value=None) for i in range(data.shape[0])]\n",
    "\n",
    "    def __call__(self, points, timesteps):\n",
    "        return np.stack([self.interp[timestep](points) for timestep in timesteps],0)\n",
    "    \n",
    "class Interp2Dslice:\n",
    "    def __init__(self, interp2dAT, dT_arr):\n",
    "        self.interp2dAT = interp2dAT\n",
    "        self.dT_arr = dT_arr\n",
    "        self.nDims = len(dT_arr)*2\n",
    "    \n",
    "    def __call__(self, points):\n",
    "        return self.interp2dAT(points, self.dT_arr)\n",
    "        \n",
    "# d_IE = data[0]['interpSE'].nDims\n",
    "# d_BE = data[0]['interpBS'][0].spline.c.shape[-1]\n",
    "# d_D = data[0]['interpSD'].nDims\n",
    "    \n",
    "class Interp1DPeriodic:\n",
    "    def __init__(self, x, y, kind = 'linear'):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.kind = kind\n",
    "        self.spline = interp1d(self.x,self.y,kind=self.kind,axis=0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.spline(x%self.x[-1])\n",
    "    \n",
    "class Interp1DPchipPeriodic:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.spline = PchipInterpolator(self.x,self.y,axis=0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.spline(x%self.x[-1])\n",
    "\n",
    "class Interp1dAcrossTimesteps:\n",
    "    def __init__(self, dataB, distance, kind='pchip'):\n",
    "        if kind == 'pchip':\n",
    "            self.interp = [Interp1DPchipPeriodic(distance, dataB[i]) for i in range(dataB.shape[0])]\n",
    "        else:\n",
    "            self.interp = [Interp1DPeriodic(distance, dataB[i],kind=kind) for i in range(dataB.shape[0])]\n",
    "\n",
    "    def __call__(self, x, timesteps):\n",
    "        return np.stack([self.interp[timestep](x) for timestep in timesteps],0)\n",
    "    \n",
    "class Interp1Dslice:\n",
    "    def __init__(self, interp1dAT, dT_arr):\n",
    "        self.interp1dAT = interp1dAT\n",
    "        self.dT_arr = dT_arr\n",
    "        self.nDims = len(dT_arr)*2\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.interp1dAT(x, self.dT_arr)\n",
    "\n",
    "# input: file_path,dT_arr\n",
    "def loadData(file_path, dT_arr):\n",
    "    data = read_am_file(file_path)\n",
    "\n",
    "    NT = data.shape[0]\n",
    "    NX = data.shape[1]\n",
    "    NY = data.shape[2]\n",
    "    Nv = data.shape[3]\n",
    "\n",
    "    x = np.linspace(0, 1, NX)\n",
    "    X,Y = np.meshgrid(x, x)\n",
    "\n",
    "    # Comon for all timesteps\n",
    "    nodes = np.stack([X.flatten(),Y.flatten()],-1)\n",
    "    idxCorner = np.array([0,NX-1,NX*(NY-1),NX*NY-1],dtype=int)\n",
    "    elementsBoundaryNodesOnly = np.array([[idxCorner[0],idxCorner[2],idxCorner[1]],[idxCorner[2],idxCorner[3],idxCorner[1]]],dtype=int)\n",
    "    areaElementsBoundaryNodesOnly = np.array([0.5,0.5])\n",
    "    isInDomain = IsInDomain([nodes[[idxCorner[0],idxCorner[1],idxCorner[3],idxCorner[2],idxCorner[0]]]])\n",
    "    idxCurves = [np.concatenate([np.arange(NX),np.arange(2*NX-1,NY*NX,NX),np.arange(NY*NX-2,(NY-1)*NX-1,-1),np.arange((NY-2)*NX,-1,-NX)])]\n",
    "    distance = [np.arange(0,idxCurve.shape[0])*1.0/(NX-1) for idxCurve in idxCurves]\n",
    "    lengthCurves = [4]\n",
    "    nodesCurves = [nodes[idxCurve] for idxCurve in idxCurves]\n",
    "    idxCurveCorner = np.array([0,0,0,0],dtype=int)\n",
    "    distanceCornerCurve = np.array([0,1,3,2],dtype=int)\n",
    "    distance2boundary = np.min(np.stack([np.abs(X),np.abs(X-1),np.abs(Y),np.abs(Y-1)],-1),-1)\n",
    "    interpD2B = RegularGridInterpolator((x,x),distance2boundary, method='linear', bounds_error=False, fill_value=None)\n",
    "    interpBC = [Interp1DPeriodic(distance[0],nodesCurves[0],kind='linear')]\n",
    "    normalCurves = [np.zeros((idxCurves[0].shape[0],2))]\n",
    "    normalCurves[0][nodesCurves[0][:,0]==0,0] = -1\n",
    "    normalCurves[0][nodesCurves[0][:,0]==1,0] = 1\n",
    "    normalCurves[0][nodesCurves[0][:,1]==0,1] = -1\n",
    "    normalCurves[0][nodesCurves[0][:,1]==1,1] = 1\n",
    "    normalCurves[0] = normalCurves[0]/np.sqrt(np.sum(normalCurves[0]**2,1,keepdims=True))\n",
    "    interpBN = [Interp1DPeriodic(distance[0],normalCurves[0],kind='linear')]\n",
    "\n",
    "    interp2dAcrossTimesteps = Interp2dAcrossTimesteps(data, x, x)\n",
    "    dataB = [data.reshape((NT,NX*NY,Nv))[:,idxCurves[0],:]]\n",
    "    interp1dAcrossTimesteps = [Interp1dAcrossTimesteps(dataB[0], distance[0], kind='pchip')]\n",
    "\n",
    "    data_processed = []\n",
    "    for i in range(-dT_arr[0],NT-dT_arr[-1]):\n",
    "        # interpSE = lambda points, idxs = dT_arr+i: interp2dAcrossTimesteps(points, idxs)\n",
    "        # interpSD = lambda points, idxs = [i]: interp2dAcrossTimesteps(points, idxs)\n",
    "        interpSE = Interp2Dslice(interp2dAcrossTimesteps, dT_arr+i)\n",
    "        interpSD = Interp2Dslice(interp2dAcrossTimesteps, [i])\n",
    "        # interpBS = [lambda x,idxs=dT_arr+i: interp1dAcrossTimesteps[0](x, idxs)]\n",
    "        interpBS = [Interp1Dslice(interp1dAcrossTimesteps[0], dT_arr+i)]\n",
    "        data_processed.append(\n",
    "            {'nodes': nodes,'elementsBoundaryNodesOnly': elementsBoundaryNodesOnly,'areaElementsBoundaryNodesOnly': areaElementsBoundaryNodesOnly,\n",
    "            'isInDomain': isInDomain,'interpSE': interpSE,'interpSD': interpSD,'interpD2B': interpD2B,'nodesCurves': nodesCurves,\n",
    "            'lengthCurves': lengthCurves,'interpBC': interpBC,'interpBS': interpBS,'interpBN': interpBN,\n",
    "            'distanceCornerCurve': distanceCornerCurve, 'idxCurveCorner': idxCurveCorner, 'dT': i}\n",
    "        )\n",
    "\n",
    "    return data_processed\n",
    "\n",
    "# file_path = r'TrainingData/TurbulentFlowData/2400.am'\n",
    "# data_processed = loadData(file_path, np.arange(-50,0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'nDims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb#W1sZmlsZQ%3D%3D?line=637'>638</a>\u001b[0m nodesIE \u001b[39m=\u001b[39m GetRadialEncoderInputMask(\u001b[39m10\u001b[39m,\u001b[39m8\u001b[39m,\u001b[39m0.5\u001b[39m,\u001b[39m0.05\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb#W1sZmlsZQ%3D%3D?line=638'>639</a>\u001b[0m nodesD \u001b[39m=\u001b[39m GetRadialEncoderInputMask(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0.5\u001b[39m,\u001b[39m0.05\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb#W1sZmlsZQ%3D%3D?line=640'>641</a>\u001b[0m dataset \u001b[39m=\u001b[39m GetInteriorDatasetFixed(data_train, nodesIE, nodesD, \u001b[39m10\u001b[39;49m, \u001b[39m10\u001b[39;49m, elSizeMin\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, elSizeMax\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb#W1sZmlsZQ%3D%3D?line=641'>642</a>\u001b[0m \u001b[39m# dataset_b = GetBoundaryDatasetFixed(data_train, nodesIE, distanceBE, nodesD, batch_size=batch_size[1], n_batches=n_batches, elSizeMin=0.1, elSizeMax=0.2)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb#W1sZmlsZQ%3D%3D?line=642'>643</a>\u001b[0m \u001b[39m# dataset_c = GetCornerDatasetFixed(data, nodesIE, distanceBE, nodesD, 10, 10, elSizeMin=0.21, elSizeMax=0.22)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb#W1sZmlsZQ%3D%3D?line=643'>644</a>\u001b[0m \u001b[39m# gen = BoundaryDatasetGenerator(data[:1], nodesIE, distanceBE, nodesD, batch_size[1], n_batches, elSizeMin=0.1, elSizeMax=0.2)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb#W1sZmlsZQ%3D%3D?line=647'>648</a>\u001b[0m \u001b[39m#         break\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb#W1sZmlsZQ%3D%3D?line=648'>649</a>\u001b[0m \u001b[39m#     i += 1\u001b[39;00m\n",
      "\u001b[1;32m/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb#W1sZmlsZQ%3D%3D?line=369'>370</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mGetInteriorDatasetFixed\u001b[39m(data, nodesIE, nodesD, batch_size, n_batches, elSizeMin\u001b[39m=\u001b[39m\u001b[39m0.04\u001b[39m, elSizeMax\u001b[39m=\u001b[39m\u001b[39m0.07\u001b[39m,sRand\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,variableElSize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb#W1sZmlsZQ%3D%3D?line=370'>371</a>\u001b[0m     d_IE \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39minterpSE\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mnDims\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb#W1sZmlsZQ%3D%3D?line=371'>372</a>\u001b[0m     d_D \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39minterpSD\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnDims\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jakubhorsky/Documents/Programming/PythonProjects/SINNs_FYP_explore/SINNs_FEM_implementation_turbulence.ipynb#W1sZmlsZQ%3D%3D?line=372'>373</a>\u001b[0m     dataset_tesor_nodesTP \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((batch_size\u001b[39m*\u001b[39mn_batches, \u001b[39m7\u001b[39m, \u001b[39m2\u001b[39m),dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'nDims'"
     ]
    }
   ],
   "source": [
    "def GenerateGeometry2(p):\n",
    "    g = cfg.Geometry()\n",
    "    for i in range(p.shape[0]):\n",
    "        g.point(list(p[i]))\n",
    "    \n",
    "    for i in range(4):\n",
    "        g.line([i,(i+1)%4],marker=1)\n",
    "    g.spline(list(range(4,p.shape[0]))+[4],marker=2)\n",
    "    g.surface([0,1,2,3],[[4]])\n",
    "    return g\n",
    "\n",
    "def MeshSurface(g,elSize):\n",
    "    mesh = cfm.GmshMesh(g)\n",
    "    mesh.elType = 2       # Degrees of freedom per node.\n",
    "    mesh.dofsPerNode = 1     # Factor that changes element sizes.\n",
    "    mesh.elSizeFactor = elSize # Element size Factor\n",
    "    nodes, edof, dofs, bdofs, elementmarkers = mesh.create()\n",
    "\n",
    "    elements = edof-1\n",
    "    boundaryNodes = [np.array(bdofs[1])-1,np.array(bdofs[2])-1]\n",
    "    internalNodes = np.setdiff1d(np.arange(nodes.shape[0]), np.concatenate(boundaryNodes))\n",
    "    return nodes, elements, boundaryNodes, internalNodes\n",
    "\n",
    "def computeLengthAlongCurve(nodesB):\n",
    "    dl = np.sqrt(((nodesB[1:]-nodesB[:-1])**2).sum(1))\n",
    "    l = np.cumsum(dl)\n",
    "    l = np.concatenate([[0],l],0)\n",
    "    return l\n",
    "\n",
    "def RemeshData(data,elSize):\n",
    "    nodes = data['nodes']\n",
    "    idxSquare = data['idxCorner']\n",
    "    idxSpline = data['idxCurves'][1][:-1]\n",
    "    nodesB = np.concatenate([ nodes[idxSquare], nodes[idxSpline]],0)\n",
    "\n",
    "    g = GenerateGeometry2(nodesB)\n",
    "    nodes,elements,idxCurves,internalNodes = MeshSurface(g,elSize)\n",
    "    for i in range(len(idxCurves)):\n",
    "        idxCurves[i] = np.sort(idxCurves[i])\n",
    "\n",
    "    nodesCurve00 = nodes[idxCurves[0][0]]\n",
    "    alpha0 = np.arctan2(nodesCurve00[1],nodesCurve00[0])\n",
    "    nodesCurves0 = nodes[idxCurves[0]]\n",
    "    alpha = np.arctan2(nodesCurves0[:,1],nodesCurves0[:,0])-alpha0\n",
    "    alpha[alpha<0] += 2*np.pi\n",
    "    idxs_sorted = np.argsort(alpha)\n",
    "    idxCurves[0] = idxCurves[0][idxs_sorted]\n",
    "    for i in range(len(idxCurves)):\n",
    "        idxCurves[i] = np.concatenate([idxCurves[i],idxCurves[i][0:1]],0)\n",
    "\n",
    "    nodesCurves = [nodes[idxCurves[i]] for i in range(len(idxCurves))]\n",
    "    distanceCurves = [computeLengthAlongCurve(nodesCurves[i]) for i in range(len(idxCurves))]\n",
    "    lengthCurves = [distanceCurves[i][-1] for i in range(len(idxCurves))]\n",
    "\n",
    "    solution = data['interpSE'](nodes)\n",
    "    for i in range(len(idxCurves)):\n",
    "        solution[idxCurves[i][:-1]] =  data['interpBS'][i](distanceCurves[i][:-1])[:,:solution.shape[-1]]\n",
    "    curvesNormals = [data['interpBN'][i](distanceCurves[i]) for i in range(len(idxCurves))]\n",
    "\n",
    "\n",
    "    data_remeshed = {'nodes':nodes, 'elements':elements, 'idxCurves':idxCurves, 'internalNodes':internalNodes, \\\n",
    "                     'nodesCurves':nodesCurves, 'distanceCurves':distanceCurves, 'lengthCurves':lengthCurves, \\\n",
    "                     'solution':solution, 'curvesNormals':curvesNormals, 'interpSE':data['interpSE'], 'interpSD':data['interpSD'], \\\n",
    "                     'interpBS':data['interpBS'], 'interpBN':data['interpBN'], 'interpBC':data['interpBC'], 'isInDomainF':data['isInDomainF']}\n",
    "\n",
    "    return data_remeshed\n",
    "\n",
    "def GetAreaTriang(nodes_els):\n",
    "    # Calculate the area of a triangles in a mesh\n",
    "    #   nodes_els - [N,3,2] tensor which containes the x,y positions of nodes of N triangles\n",
    "    b = (np.roll(nodes_els[:,:,1],1,axis=1) - np.roll(nodes_els[:,:,1],2,axis=1)).reshape(-1,3,1)\n",
    "    c = (np.roll(nodes_els[:,:,0],2,axis=1) - np.roll(nodes_els[:,:,0],1,axis=1)).reshape(-1,3,1)\n",
    "    Area = np.abs(np.matmul(nodes_els[:,:,0].reshape(-1,1,3),b))/2\n",
    "    return Area\n",
    "\n",
    "def GenerateTriangTrainMeshInter(sRand=0.1):\n",
    "    # Generates random variation of a mesh made out of 6 triangular elements in a hexagon configuration\n",
    "    # The mesh is normalized so that the longest edge length is 1\n",
    "    #   nodes - [7,2] tensor which containes the x,y positions of nodes of the mesh\n",
    "    #   elements - [6,3] tensor which containes the indices of the nodes of the mesh\n",
    "\n",
    "    # constructu a hexagon with a point in the middle\n",
    "    alphas = np.linspace(0,2*np.pi,7)[:-1]-2*np.pi/3 + np.random.uniform(-np.pi/15,np.pi/15,6)\n",
    "    r = np.random.uniform(1-sRand,1+sRand,6)\n",
    "    bNodes = np.stack([np.cos(alphas)*r,np.sin(alphas)*r],axis=1)\n",
    "    nodes = np.concatenate([[[0,0]],bNodes],axis=0)\n",
    "\n",
    "    # construct element matrix\n",
    "    elements = np.arange(3).reshape(1,-1)+np.arange(6).reshape(-1,1)\n",
    "    elements[:,0] = 0\n",
    "    elements[elements>6]=1\n",
    "\n",
    "    # randomly rotate the mesh\n",
    "    rot = np.random.uniform(0,np.pi/3)\n",
    "    rot_mat = np.array([[np.cos(rot),np.sin(rot)],[-np.sin(rot),np.cos(rot)]])\n",
    "    nodes = (rot_mat@nodes.T).T\n",
    "    return nodes,elements\n",
    "\n",
    "def RandomTriangSample(nodes):\n",
    "    # Sample a random points inside a trinagle defined by [3,2] matrix: nodes\n",
    "    y = np.random.uniform()\n",
    "    x = np.abs(np.random.uniform()-np.random.uniform())\n",
    "    p = nodes[1] + (nodes[0]-nodes[1])*x - (nodes[1]-nodes[2])*(1-x)*y\n",
    "    return p\n",
    "\n",
    "def RandomMeshSample(nodes,elements,Area):\n",
    "    # Sample a random point from a 2D shape defined with a mesh: (nodes, elements)\n",
    "    el_sample = np.random.choice(np.arange(elements.shape[0]),p=Area.flatten()/Area.sum())\n",
    "    point_sample = RandomTriangSample(nodes[elements[el_sample]])\n",
    "    return point_sample\n",
    "\n",
    "# def GetRandomTPmesh(nodes,elements,areaElements,isInDomainF,interpD2B,elSizeMin=0.04,elSizeMax=0.07,sRand=0.1):\n",
    "#     # Get a random interior training patch mesh from a mesh defined by (nodes, elements, boundaryNodes)\n",
    "#     # The mesh will be scaled so that the longest edge is approximately between elSizeMin and elSizeMax\n",
    "#     # In case the randomly sampled point is so close to the boundary that the Training Patch mesh does not fit fully inside the domain the point is resampled\n",
    "#     iterations = 0\n",
    "#     while (iterations < 1_000):\n",
    "#         samplePoint = RandomMeshSample(nodes,elements,areaElements)\n",
    "#         notesTP, elementsTP = GenerateTriangTrainMeshInter(sRand)\n",
    "#         elSize = np.random.uniform(elSizeMin, elSizeMax)\n",
    "#         if interpD2B(samplePoint.reshape(-1,2)) < elSize*1.5:\n",
    "#             iterations += 1\n",
    "#             continue\n",
    "#         nodesTP = notesTP * elSize + samplePoint.reshape(1,2)\n",
    "#         isInDomain = isInDomainF(nodesTP)\n",
    "#         if np.all(isInDomain):\n",
    "#             break\n",
    "#         iterations += 1\n",
    "#     return nodesTP, elementsTP\n",
    "\n",
    "def GetRandomTPmesh(nodes,elements,areaElements,isInDomainF,interpD2B,elSizeMin=0.04,elSizeMax=0.07,elSizeF=None,sRand=0.1):\n",
    "    # Get a random interior training patch mesh from a mesh defined by (nodes, elements, boundaryNodes)\n",
    "    # The mesh will be scaled so that the longest edge is approximately between elSizeMin and elSizeMax\n",
    "    # In case the randomly sampled point is so close to the boundary that the Training Patch mesh does not fit fully inside the domain the point is resampled\n",
    "    iterations = 0\n",
    "    while (iterations < 1_000):\n",
    "        samplePoint = RandomMeshSample(nodes,elements,areaElements).reshape(-1,2)\n",
    "        notesTP, elementsTP = GenerateTriangTrainMeshInter(sRand)\n",
    "        if elSizeF is None:\n",
    "            elSize = np.random.uniform(elSizeMin, elSizeMax)\n",
    "        else:\n",
    "            elSize = elSizeF(samplePoint) * np.random.uniform(elSizeMin, elSizeMax)\n",
    "        if interpD2B(samplePoint) < elSize*1.6:\n",
    "            iterations += 1\n",
    "            continue\n",
    "        nodesTP = notesTP * elSize + samplePoint\n",
    "        isInDomain = isInDomainF(nodesTP)\n",
    "        if np.all(isInDomain):\n",
    "            break\n",
    "        iterations += 1\n",
    "    return nodesTP, elementsTP\n",
    "\n",
    "# Defined the points of the encoder input mask\n",
    "def GetRadialEncoderInputMask(n_r,n_theta,k=0.5,Esize=0.05):\n",
    "    # Get Radial interior encoder input mask\n",
    "    #   n_theta - number of radial directions along which points are defined\n",
    "    #   n_r - number of points along each radial direction\n",
    "    #   k - coefficient between 0 and 1 which defines how much should the points be denser towards the centre compared to the edge\n",
    "    #   Esize - size of the interour encoder mask, radius of the circle\n",
    "    alphas = np.linspace(0,2*np.pi,n_theta+1)[:-1].reshape(1,-1)\n",
    "    r = np.linspace(0,1,n_r+1)[1:].reshape(-1,1)\n",
    "    r = k*r+(1-k)*r**3\n",
    "    nodesIE = np.stack([r*np.cos(alphas),r*np.sin(alphas)],axis=-1)\n",
    "    nodesIE = np.concatenate([[[0,0]],nodesIE.reshape(-1,2)],axis=0)\n",
    "    return nodesIE*Esize\n",
    "\n",
    "def GetSquareEncoderInputMask(n,k=0.5,Esize=0.05):\n",
    "    # Get Square interior encoder input mask\n",
    "    #   The mask is a 2n x 2n grid points of size 2*Esize x 2*Esize (in x,y coordinates)\n",
    "    #   k - coefficient between 0 and 1 which defines how much should the points be denser towards the centre compared to the edge\n",
    "    x = np.linspace(-1,1,2*n+1)\n",
    "    x = k*x+(1-k)*x**3\n",
    "    X,Y = np.meshgrid(x,x)\n",
    "    nodesIE = np.stack([X,Y],axis=-1).reshape(-1,2)\n",
    "    return nodesIE*Esize\n",
    "\n",
    "def GetBoundaryEncoderInputMask(n,k,Esize=0.05):\n",
    "    # Get boundary encoder input mask whcih is defined along the boundary\n",
    "    #   total number of points in teh boundary encoder input mask is 2*n-1\n",
    "    #   k - coefficient between 0 and 1 which defines how much should the points be denser towards the centre compared to the edge\n",
    "    #   Esize - the size of the  boundary encoder input mask, the length of the mask is 2*Esize\n",
    "    x = np.linspace(-1,1,2*n-1)\n",
    "    x = (k*np.abs(x)+(1-k)*np.abs(x)**2) * np.sign(x)\n",
    "    return x*Esize\n",
    "\n",
    "def GetEncoderInputTP(nodesTP,nodesIE):\n",
    "    # Based on training patch mesh nodes (nodesTP) and nodes of the interior encoder input mask (nodesIE) generates all interior encoder input mask points within the training patch\n",
    "    nodesTP = nodesTP.reshape(-1,1,2)\n",
    "    nodesIE = nodesIE.reshape(1,-1,2)\n",
    "    nodesTP_IE = nodesTP + nodesIE\n",
    "    return nodesTP_IE\n",
    "\n",
    "def GetRandomTrainingPatchInternal(data, nodesIE, nodesD, elSizeMin=0.04, elSizeMax=0.07, debugging=False,sRand=0.1,variableElSize=False):\n",
    "    # Samples a random training patch from the dataset of solutions and generates all the relevent outputs\n",
    "    #   data - list of dictionaries which defines dataset of solutions\n",
    "    #   nodesIE - interior encoder input mask nodes\n",
    "    #   nodesD - decoder output mask nodes\n",
    "    #   elSizeMin, elSizeMax - approximate min, max size of the logest edge in the training patch mesh\n",
    "    idx = np.random.randint(len(data))\n",
    "    nodes = data[idx]['nodes']\n",
    "    # if variableElSize:\n",
    "    #     elements = data[idx]['elements']\n",
    "    #     areaElements = np.ones(elements.shape[0])\n",
    "    # else:\n",
    "    elements = data[idx]['elementsBoundaryNodesOnly']\n",
    "    areaElements = data[idx]['areaElementsBoundaryNodesOnly']\n",
    "    interpSE = data[idx]['interpSE']\n",
    "    interpSD = data[idx]['interpSD']\n",
    "    isInDomainF = data[idx]['isInDomainF']\n",
    "    interpD2B = data[idx]['interpD2B']\n",
    "    if variableElSize:\n",
    "        interpELS = data[idx]['interpELS']\n",
    "    else:\n",
    "        interpELS = None\n",
    "\n",
    "    # nodesTP, elementsTP = GetRandomTPmesh(nodes,elements,areaElements,isInDomainF,elSizeMin,elSizeMax,sRand=sRand) # get training patch mesh\n",
    "    nodesTP, elementsTP = GetRandomTPmesh(nodes,elements,areaElements,isInDomainF,interpD2B,elSizeMin=elSizeMin,elSizeMax=elSizeMax,elSizeF=interpELS,sRand=sRand)\n",
    "    nodesTP_IE = GetEncoderInputTP(nodesTP,nodesIE) # get all nodes of the training patch\n",
    "    uTP_IE = interpSE(nodesTP_IE.reshape(-1,2)).reshape(7,nodesIE.shape[0],-1) # sample solution at training patch nodes\n",
    "    bTP_IE = isInDomainF(nodesTP_IE.reshape(-1,2)).reshape(7,nodesIE.shape[0]) # sample boundary mask for interior econder input (1 if inside the domain, 0 if outside)\n",
    "    uTP_IE[~bTP_IE] = 0 # set solution values outside of the domain to 0\n",
    "    bTP_IE = np.expand_dims(bTP_IE,-1)\n",
    "\n",
    "    nodesTP_D = nodesD+nodesTP[:1,:]\n",
    "    u_D = interpSD(nodesTP_D) # sample solution at decoder output mask nodes\n",
    "    b_D = isInDomainF(nodesTP_D) # sample boundary mask for decoder output (1 if inside the domain, 0 if outside)\n",
    "    u_D[~b_D] = 0 # set solution values outside of the domain to 0\n",
    "\n",
    "    if debugging:\n",
    "        return nodesTP, elementsTP, uTP_IE, bTP_IE, u_D, b_D, nodesTP_IE\n",
    "    else:\n",
    "        return nodesTP, elementsTP, uTP_IE, bTP_IE, u_D, b_D\n",
    "\n",
    "def GenerateTriangTrainMeshBoundary(v_rand = 0.1):\n",
    "    # Generates a mesh of 6 triangular elements in a hexagon configuration where the two bottom points have x,y coordinates (0,0) and (1,0)\n",
    "    alphas = np.linspace(0,2*np.pi,7)[:-1]-2*np.pi/3\n",
    "    bNodes = np.stack([np.cos(alphas),np.sin(alphas)],axis=1)\n",
    "    nodes = np.concatenate([[[0,0]],bNodes],axis=0)\n",
    "    # randomly perturb the nodes except the two bottom ones\n",
    "    v_rand_nodes = np.random.uniform(-v_rand,v_rand,(7,2))\n",
    "    v_rand_nodes[[1,2],:] = 0\n",
    "    nodes += v_rand_nodes\n",
    "    nodes = nodes-nodes[1]\n",
    "    elements = np.arange(3).reshape(1,-1)+np.arange(6).reshape(-1,1)\n",
    "    elements[:,0] = 0\n",
    "    elements[elements>6]=1\n",
    "    return nodes,elements\n",
    "\n",
    "def GetRandomTPmeshBoundary(nodesB,length,interpNodesB,IsInDomainF,elSizeMin=0.04,elSizeMax=0.07,elSizeBF=None,idx_curve_0=None,distance_0=None):\n",
    "    # Get a random boundary training patch mesh (nodesTPB, elementsTPB) from a mesh defined by (nodesB, alpha)\n",
    "    #   nodesB - [N,2] tensor which contains the x,y coordinates of the boundary nodes\n",
    "\n",
    "    length = np.array(length)\n",
    "    if idx_curve_0 is None:\n",
    "        # idx_curve_0 = np.random.choice(np.arange(len(nodesB)),p=length/length.sum())\n",
    "        idx_curve_0 = np.random.randint(0,len(nodesB))\n",
    "    if distance_0 is None:\n",
    "        distance_0 = np.random.uniform(0,length[idx_curve_0]) # randomly sample a point on the boundary\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    while iterations < 1_000:\n",
    "\n",
    "        if elSizeBF is None:\n",
    "            elSize = np.random.uniform(elSizeMin,elSizeMax) # define the element size of the training patch mesh\n",
    "        else:\n",
    "            elSize = elSizeBF[idx_curve_0](distance_0) #* np.random.uniform(elSizeMin,elSizeMax)\n",
    "\n",
    "        scale = 1 # scaling factor elSize which is used to fite the training patch mesh in areas of high boundary curvature were distance is not a good measure of the element size\n",
    "        for i in range(100):\n",
    "            # iterate to find the second point of the training patch mesh on the boundary\n",
    "            distance_1 = (distance_0 + elSize*scale) # define the second point of the training patch mesh on the boundary\n",
    "\n",
    "            # interpolate the boundary nodes between the two sampled points\n",
    "            nodes_0 = interpNodesB[idx_curve_0](distance_0)\n",
    "            nodes_1 = interpNodesB[idx_curve_0](distance_1)\n",
    "\n",
    "            # Computes the appropriate scaling and rotation for the normalized training patch mesh\n",
    "            rotTPB = -np.arctan2(nodes_1[1]-nodes_0[1],nodes_1[0]-nodes_0[0])\n",
    "            elSizeTPB = np.sqrt(((nodes_1 - nodes_0)**2).sum())\n",
    "            if abs(elSizeTPB-elSize)/elSize < 0.1:\n",
    "                break\n",
    "            \n",
    "            scale = scale*np.sqrt(elSize/elSizeTPB)\n",
    "\n",
    "        rot_mat = np.array([[np.cos(rotTPB),np.sin(rotTPB)],[-np.sin(rotTPB),np.cos(rotTPB)]])\n",
    "        nodesTPB, elementsTPB = GenerateTriangTrainMeshBoundary()\n",
    "        nodesTPB = (rot_mat@nodesTPB.T).T * elSizeTPB + nodes_0\n",
    "        isInDomain = IsInDomainF(nodesTPB[[0,3,4,5,6]])\n",
    "        \n",
    "        # Check if the training patch mesh is within the domain and the element size is within the bounds\n",
    "        if np.all(isInDomain):\n",
    "            break\n",
    "        iterations += 1\n",
    "        distance_0 = np.random.uniform(0,length[idx_curve_0])# randomly sample a point on the boundary\n",
    "\n",
    "    return nodesTPB, elementsTPB, distance_0, distance_1, idx_curve_0\n",
    "\n",
    "\n",
    "def GetRandomTrainingPatchBoundary(data, nodesIE, distanceBE, nodesD, elSizeMin=0.04, elSizeMax=0.07, elSizeBF=None, debugging=False):\n",
    "    # Samples a random boundary training patch (2 nodes lie on the boundary) from the dataset of solutions and generates all the relevent outputs\n",
    "    #   data - list of dictionaries which defines dataset of solutions\n",
    "    #   nodesIE - interior encoder input mask nodes\n",
    "    #   s_alphaBE - boundary encoder input mask nodes (defined as distance along the boundary)\n",
    "    #   elSizeMin, elSizeMax - approximate min, max size of the logest edge in the training patch mesh\n",
    "    idx = np.random.randint(len(data))\n",
    "    nodesB = data[idx]['nodesCurves']\n",
    "    lengthCurves = data[idx]['lengthCurves']\n",
    "    interpNodesB = data[idx]['interpBC']\n",
    "    isInDomainF = data[idx]['isInDomainF']\n",
    "    if elSizeBF is not None:\n",
    "        elSizeBF = [lambda x: 0.3, lambda x: 0.01]\n",
    "\n",
    "    # Get random training patch mesh\n",
    "    # nodesTPB, elementsTPB, distance_0, distance_1, idx_curve_0 = GetRandomTPmeshBoundary(nodesB,lengthCurves,interpNodesB,isInDomainF,elSizeMin,elSizeMax)\n",
    "    nodesTPB, elementsTPB, distance_0, distance_1, idx_curve_0 = GetRandomTPmeshBoundary(nodesB,lengthCurves,interpNodesB,isInDomainF,elSizeMin=elSizeMin,elSizeMax=elSizeMax,elSizeBF=elSizeBF)\n",
    "\n",
    "    # Get interior encoder and boundary encoder intpus\n",
    "    #  interiour encoder inputs: uTP_BEi (physical variabel u values), bTP_BEi (is inside the domain)\n",
    "    #  boundary encoder inputs: uTP_BEb (physical variabel u values), normalTP_BEb (normal vector of the boundary)\n",
    "    interpSE = data[idx]['interpSE']\n",
    "    interpSD = data[idx]['interpSD']\n",
    "    interpBS = data[idx]['interpBS'][idx_curve_0]\n",
    "    interpBN = data[idx]['interpBN'][idx_curve_0]\n",
    "\n",
    "    nodesTP_BEi = GetEncoderInputTP(nodesTPB[[0,3,4,5,6]],nodesIE) # get nodes for the interour encoder input\n",
    "    uTP_BEi = interpSE(nodesTP_BEi.reshape(-1,2)).reshape(nodesTP_BEi.shape[0],nodesTP_BEi.shape[1],-1)\n",
    "    bTP_BEi = isInDomainF(nodesTP_BEi.reshape(-1,2)).reshape(nodesTP_BEi.shape[0],nodesTP_BEi.shape[1])\n",
    "    uTP_BEi[~bTP_BEi] = 0\n",
    "    bTP_BEi = np.expand_dims(bTP_BEi,-1)\n",
    "    distanceTP_BEb = np.array([[distance_0],[distance_1]])+distanceBE # get alpha for the boundary encoder input\n",
    "    uTP_BEb = interpBS(distanceTP_BEb)\n",
    "    normalTP_BEb = interpBN(distanceTP_BEb)\n",
    "\n",
    "    nodesTP_D = nodesD+nodesTPB[:1,:]\n",
    "    u_D = interpSD(nodesTP_D) # sample solution at decoder output mask nodes\n",
    "    b_D = isInDomainF(nodesTP_D) # sample boundary mask for decoder output (1 if inside the domain, 0 if outside)\n",
    "    u_D[~b_D] = 0 # set solution values outside of the domain to 0\n",
    "\n",
    "    if debugging:\n",
    "        interpBC = data[idx]['interpBC'][idx_curve_0]\n",
    "        nodesTP_BEb = interpBC(distanceTP_BEb)\n",
    "        return nodesTPB, elementsTPB, uTP_BEi, bTP_BEi, uTP_BEb, normalTP_BEb, u_D, b_D, nodesTP_BEi, nodesTP_BEb\n",
    "    else:\n",
    "        return nodesTPB, elementsTPB, uTP_BEi, bTP_BEi, uTP_BEb, normalTP_BEb, u_D, b_D\n",
    "\n",
    "def InteriorDatasetGenerator(data, nodesIE, nodesD, batch_size, n_batches, elSizeMin=0.04, elSizeMax=0.07,sRand=0.1):\n",
    "    for i in range(batch_size*n_batches):\n",
    "        nodesTP, elementsTP, uTP_IE, bTP_IE, u_D, b_D = GetRandomTrainingPatchInternal(data, nodesIE, nodesD, elSizeMin=elSizeMin, elSizeMax=elSizeMax,sRand=sRand)\n",
    "        nodesTP = tf.convert_to_tensor(nodesTP, dtype=tf.float32)\n",
    "        elementsTP = tf.convert_to_tensor(elementsTP, dtype=tf.int32)\n",
    "        uTP_IE = tf.convert_to_tensor(uTP_IE, dtype=tf.float32)\n",
    "        bTP_IE = tf.convert_to_tensor(bTP_IE, dtype=tf.float32)\n",
    "        u_D = tf.convert_to_tensor(u_D, dtype=tf.float32)\n",
    "        b_D = tf.convert_to_tensor(b_D, dtype=tf.float32)\n",
    "        yield nodesTP, elementsTP, uTP_IE, bTP_IE, u_D, b_D\n",
    "\n",
    "def GetInteriorDataset(data, nodesIE, nodesD, batch_size, n_batches, elSizeMin=0.04, elSizeMax=0.07):\n",
    "    d_IE = data[0]['interpSE'].nDims\n",
    "    d_D = data[0]['interpSD'].nDims\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: InteriorDatasetGenerator(data, nodesIE, nodesD, batch_size, n_batches, elSizeMin=elSizeMin, elSizeMax=elSizeMax),\n",
    "        output_types=(tf.float32, tf.int32, tf.float32, tf.float32, tf.float32, tf.float32),\n",
    "        output_shapes=(tf.TensorShape([7,2]), tf.TensorShape([6,3]), \n",
    "                       tf.TensorShape([7,nodesIE.shape[0],d_IE]), tf.TensorShape([7,nodesIE.shape[0],1]), \n",
    "                       tf.TensorShape([nodesD.shape[0],d_D]), tf.TensorShape([nodesD.shape[0],1])))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def GetInteriorDatasetFixed(data, nodesIE, nodesD, batch_size, n_batches, elSizeMin=0.04, elSizeMax=0.07,sRand=0.1,variableElSize=False):\n",
    "    d_IE = data[0]['interpSE'].nDims\n",
    "    d_D = data[0]['interpSD'].nDims\n",
    "    dataset_tesor_nodesTP = np.zeros((batch_size*n_batches, 7, 2),dtype=np.float32)\n",
    "    dataset_tesor_elementsTP = np.zeros((batch_size*n_batches, 6, 3),dtype=np.int32)\n",
    "    dataset_tesor_uTP_IE = np.zeros((batch_size*n_batches, 7, nodesIE.shape[0], d_IE),dtype=np.float32)\n",
    "    dataset_tesor_bTP_IE = np.zeros((batch_size*n_batches, 7, nodesIE.shape[0], 1),dtype=np.float32)\n",
    "    dataset_tesor_u_D = np.zeros((batch_size*n_batches, nodesD.shape[0], d_D),dtype=np.float32)\n",
    "    dataset_tesor_b_D = np.zeros((batch_size*n_batches, nodesD.shape[0], 1),dtype=np.float32)\n",
    "    for i in tqdm(range(batch_size*n_batches)):\n",
    "        nodesTP, elementsTP, uTP_IE, bTP_IE, u_D, b_D = GetRandomTrainingPatchInternal(data, nodesIE, nodesD, elSizeMin=elSizeMin, elSizeMax=elSizeMax,sRand=sRand,variableElSize=variableElSize)\n",
    "        dataset_tesor_nodesTP[i] = nodesTP\n",
    "        dataset_tesor_elementsTP[i] = elementsTP\n",
    "        dataset_tesor_uTP_IE[i] = uTP_IE\n",
    "        dataset_tesor_bTP_IE[i] = bTP_IE\n",
    "        dataset_tesor_u_D[i] = u_D\n",
    "        dataset_tesor_b_D[i] = b_D\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataset_tesor_nodesTP, dataset_tesor_elementsTP, \n",
    "                                                  dataset_tesor_uTP_IE, dataset_tesor_bTP_IE, \n",
    "                                                  dataset_tesor_u_D, dataset_tesor_b_D))\n",
    "\n",
    "    dataset = dataset.shuffle(batch_size*n_batches).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def BoundaryDatasetGenerator(data, nodesIE, distanceBE, nodesD, batch_size, n_batches, elSizeMin=0.04, elSizeMax=0.07, elSizeBF=None):\n",
    "    for i in range(batch_size*n_batches):\n",
    "        nodesTPB, elementsTPB, uTP_BEi, bTP_BEi, uTP_BEb, normalTP_BEb, u_D, b_D = GetRandomTrainingPatchBoundary(data, nodesIE, distanceBE, nodesD, elSizeMin=elSizeMin, elSizeMax=elSizeMax, elSizeBF=elSizeBF)\n",
    "        nodesTPB = tf.convert_to_tensor(nodesTPB, dtype=tf.float32)\n",
    "        elementsTPB = tf.convert_to_tensor(elementsTPB, dtype=tf.int32)\n",
    "        uTP_BEi = tf.convert_to_tensor(uTP_BEi, dtype=tf.float32)\n",
    "        bTP_BEi = tf.convert_to_tensor(bTP_BEi, dtype=tf.float32)\n",
    "        uTP_BEb = tf.convert_to_tensor(uTP_BEb, dtype=tf.float32)\n",
    "        normalTP_BEb = tf.convert_to_tensor(normalTP_BEb, dtype=tf.float32)\n",
    "        u_D = tf.convert_to_tensor(u_D, dtype=tf.float32)\n",
    "        b_D = tf.convert_to_tensor(b_D, dtype=tf.float32)\n",
    "        yield nodesTPB, elementsTPB, uTP_BEi, bTP_BEi, uTP_BEb, normalTP_BEb, u_D, b_D\n",
    "\n",
    "def GetBoundaryDataset(data, nodesIE, distanceBE, nodesD, batch_size, n_batches, elSizeMin=0.04, elSizeMax=0.07):\n",
    "    d_IE = data[0]['interpSE'].nDims\n",
    "    d_BE = data[0]['interpBS'][0].spline.c.shape[-1]\n",
    "    d_D = data[0]['interpSD'].nDims\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: BoundaryDatasetGenerator(data, nodesIE, distanceBE, nodesD, batch_size, n_batches, elSizeMin=elSizeMin, elSizeMax=elSizeMax),\n",
    "        output_types=(tf.float32, tf.int32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32),\n",
    "        output_shapes=(tf.TensorShape([7,2]), tf.TensorShape([6,3]), \n",
    "                       tf.TensorShape([5,nodesIE.shape[0],d_IE]), tf.TensorShape([5,nodesIE.shape[0],1]), \n",
    "                       tf.TensorShape([2,distanceBE.shape[0],d_BE]), tf.TensorShape([2,distanceBE.shape[0],2]), \n",
    "                       tf.TensorShape([nodesD.shape[0],d_D]), tf.TensorShape([nodesD.shape[0],1])))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def GetBoundaryDatasetFixed(data, nodesIE, distanceBE, nodesD, batch_size, n_batches, elSizeMin=0.04, elSizeMax=0.07, elSizeBF=None):\n",
    "    d_IE = data[0]['interpSE'].nDims\n",
    "    d_BE = data[0]['interpBS'][0].nDims\n",
    "    d_D = data[0]['interpSD'].nDims\n",
    "\n",
    "    dataset_tesor_nodesTPB = np.zeros((batch_size*n_batches, 7, 2),dtype=np.float32)\n",
    "    dataset_tesor_elementsTPB = np.zeros((batch_size*n_batches, 6, 3),dtype=np.int32)\n",
    "    dataset_tesor_uTP_BEi = np.zeros((batch_size*n_batches, 5, nodesIE.shape[0], d_IE),dtype=np.float32)\n",
    "    dataset_tesor_bTP_BEi = np.zeros((batch_size*n_batches, 5, nodesIE.shape[0], 1),dtype=np.float32)\n",
    "    dataset_tesor_uTP_BEb = np.zeros((batch_size*n_batches, 2, distanceBE.shape[0], d_BE),dtype=np.float32)\n",
    "    dataset_tesor_normalTP_BEb = np.zeros((batch_size*n_batches, 2, distanceBE.shape[0], 2),dtype=np.float32)\n",
    "    dataset_tesor_u_D = np.zeros((batch_size*n_batches, nodesD.shape[0], d_D),dtype=np.float32)\n",
    "    dataset_tesor_b_D = np.zeros((batch_size*n_batches, nodesD.shape[0], 1),dtype=np.float32)\n",
    "    for i in tqdm(range(batch_size*n_batches)):\n",
    "        nodesTPB, elementsTPB, uTP_BEi, bTP_BEi, uTP_BEb, normalTP_BEb, u_D, b_D = GetRandomTrainingPatchBoundary(data, nodesIE, distanceBE, nodesD, elSizeMin=elSizeMin, elSizeMax=elSizeMax, elSizeBF=elSizeBF)\n",
    "        dataset_tesor_nodesTPB[i] = nodesTPB\n",
    "        dataset_tesor_elementsTPB[i] = elementsTPB\n",
    "        dataset_tesor_uTP_BEi[i] = uTP_BEi\n",
    "        dataset_tesor_bTP_BEi[i] = bTP_BEi\n",
    "        dataset_tesor_uTP_BEb[i] = uTP_BEb\n",
    "        dataset_tesor_normalTP_BEb[i] = normalTP_BEb\n",
    "        dataset_tesor_u_D[i] = u_D\n",
    "        dataset_tesor_b_D[i] = b_D\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataset_tesor_nodesTPB, dataset_tesor_elementsTPB,\n",
    "                                                    dataset_tesor_uTP_BEi, dataset_tesor_bTP_BEi,\n",
    "                                                    dataset_tesor_uTP_BEb, dataset_tesor_normalTP_BEb,\n",
    "                                                    dataset_tesor_u_D, dataset_tesor_b_D))\n",
    "    \n",
    "    dataset = dataset.shuffle(batch_size*n_batches).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def GenerateTriangTrainMeshCorner(v_rand = 0.1):\n",
    "    # Generates a mesh of 6 triangular elements in a hexagon configuration where the 3 nodes in the left bottom corner have coordinates (0,1), (0,0) and (1,0)\n",
    "    sqrt_2 = np.sqrt(2)\n",
    "    nodes = np.array([[(sqrt_2+1)/(2*sqrt_2), (sqrt_2+1)/(2*sqrt_2)],\n",
    "                      [0,0],\n",
    "                      [1,0],\n",
    "                      [1+1/sqrt_2,1/sqrt_2],\n",
    "                      [1+1/sqrt_2,1+1/sqrt_2],\n",
    "                      [1/sqrt_2,1+1/sqrt_2],\n",
    "                      [0,1]])\n",
    "    \n",
    "    nodes[[0,3,4,5]] += np.random.uniform(-v_rand,v_rand,(4,2))\n",
    "\n",
    "    elements = np.arange(3).reshape(1,-1)+np.arange(6).reshape(-1,1)\n",
    "    elements[:,0] = 0\n",
    "    elements[elements>6]=1\n",
    "    return nodes,elements\n",
    "\n",
    "def GetRandomTPmeshCorner(distaceCornerCurve,idxCurveCorner,interpNodesB,IsInDomainF,elSizeMin=0.04,elSizeMax=0.07,sRand=0.1,idx_corner=None):\n",
    "    # Get a random boundary training patch mesh (nodesTPB, elementsTPB) from a mesh defined by (nodesB, alpha)\n",
    "    #   nodesB - [N,2] tensor which contains the x,y coordinates of the boundary nodes\n",
    "\n",
    "    if idx_corner is None:\n",
    "        idx_corner = np.random.randint(0,len(distaceCornerCurve))\n",
    "    \n",
    "    idx_curve = idxCurveCorner[idx_corner]\n",
    "    distance_1 = distaceCornerCurve[idx_corner] # distance along the boundary of the node[1]\n",
    "\n",
    "    iterations = 0\n",
    "\n",
    "    while iterations < 1_000:\n",
    "\n",
    "        elSize1 = np.random.uniform(elSizeMin,elSizeMax) # define the element size of the training patch mesh\n",
    "        elSize2 = np.random.uniform(elSize1*0.8,elSize1*1.2) # define the element size of the training patch mesh\n",
    "        distance_2 = (distance_1 + elSize1) # distance along the boundary of the node[2]\n",
    "        distance_6 = (distance_1 - elSize2) # distance along the boundary of the node[6]\n",
    "\n",
    "\n",
    "        # interpolate the boundary nodes\n",
    "        nodes_1 = interpNodesB[idx_curve](distance_1)\n",
    "        nodes_2 = interpNodesB[idx_curve](distance_2)\n",
    "        nodes_6 = interpNodesB[idx_curve](distance_6)\n",
    "\n",
    "        # compute the transformation matrix\n",
    "        R = np.stack([nodes_2-nodes_1,nodes_6-nodes_1],1)\n",
    "\n",
    "        # generate the training patch mesh and transform it so that fits the corner\n",
    "        nodesTPC, elementsTPC = GenerateTriangTrainMeshCorner(sRand)\n",
    "        nodesTPC = (R@nodesTPC.T).T + nodes_1\n",
    "        isInDomain = IsInDomainF(nodesTPC[[0,3,4,5]])\n",
    "        # break\n",
    "        if np.all(isInDomain):\n",
    "            break\n",
    "        iterations += 1\n",
    "        print(iterations)\n",
    "\n",
    "    # print(iterations)\n",
    "    return nodesTPC, elementsTPC, distance_1, distance_2, distance_6, idx_curve\n",
    "\n",
    "def GetRandomTrainingPatchCorner(data, nodesIE, distanceBE, nodesD, elSizeMin=0.04, elSizeMax=0.07,sRand=0.1, debugging=False):\n",
    "    # Samples a random boundary training patch (2 nodes lie on the boundary) from the dataset of solutions and generates all the relevent outputs\n",
    "    #   data - list of dictionaries which defines dataset of solutions\n",
    "    #   nodesIE - interior encoder input mask nodes\n",
    "    #   s_alphaBE - boundary encoder input mask nodes (defined as distance along the boundary)\n",
    "    #   elSizeMin, elSizeMax - approximate min, max size of the logest edge in the training patch mesh\n",
    "    idx = np.random.randint(len(data))\n",
    "\n",
    "    distaceCornerCurve = data[idx]['distaceCornerCurve']\n",
    "    idxCurveCorner = data[idx]['idxCurveCorner']\n",
    "    interpNodesB = data[idx]['interpBC']\n",
    "    isInDomainF = data[idx]['isInDomainF']\n",
    "    nodesTPC, elementsTPC, distance_1, distance_2, distance_6, idx_curve = GetRandomTPmeshCorner(distaceCornerCurve,idxCurveCorner,interpNodesB,isInDomainF,elSizeMin,elSizeMax,sRand=sRand)\n",
    "\n",
    "    # Get interior encoder and boundary encoder intpus\n",
    "    #  interiour encoder inputs: uTP_BEi (physical variabel u values), bTP_BEi (is inside the domain)\n",
    "    #  boundary encoder inputs: uTP_BEb (physical variabel u values), normalTP_BEb (normal vector of the boundary)\n",
    "    interpSE = data[idx]['interpSE']\n",
    "    interpSD = data[idx]['interpSD']\n",
    "    interpBS = data[idx]['interpBS'][idx_curve]\n",
    "    interpBN = data[idx]['interpBN'][idx_curve]\n",
    "\n",
    "    nodesTP_CEi = GetEncoderInputTP(nodesTPC[[0,3,4,5]],nodesIE) # get nodes for the interour encoder input\n",
    "    uTP_CEi = interpSE(nodesTP_CEi.reshape(-1,2)).reshape(nodesTP_CEi.shape[0],nodesTP_CEi.shape[1],-1)\n",
    "    bTP_CEi = isInDomainF(nodesTP_CEi.reshape(-1,2)).reshape(nodesTP_CEi.shape[0],nodesTP_CEi.shape[1])\n",
    "    uTP_CEi[~bTP_CEi] = 0\n",
    "    bTP_CEi = np.expand_dims(bTP_CEi,-1)\n",
    "    distanceTP_CEb = np.array([[distance_1],[distance_2],[distance_6]])+distanceBE # get alpha for the boundary encoder input\n",
    "    uTP_CEb = interpBS(distanceTP_CEb)\n",
    "    normalTP_CEb = interpBN(distanceTP_CEb)\n",
    "\n",
    "    nodesTP_D = nodesD+nodesTPC[:1,:]\n",
    "    u_D = interpSD(nodesTP_D) # sample solution at decoder output mask nodes\n",
    "    b_D = isInDomainF(nodesTP_D) # sample boundary mask for decoder output (1 if inside the domain, 0 if outside)\n",
    "    u_D[~b_D] = 0 # set solution values outside of the domain to 0\n",
    "\n",
    "    if debugging:\n",
    "        interpBC = data[idx]['interpBC'][idx_curve]\n",
    "        nodesTP_CEb = interpBC(distanceTP_CEb)\n",
    "        return nodesTPC, elementsTPC, uTP_CEi, bTP_CEi, uTP_CEb, normalTP_CEb, u_D, b_D, nodesTP_CEi, nodesTP_CEb\n",
    "    else:\n",
    "        return nodesTPC, elementsTPC, uTP_CEi, bTP_CEi, uTP_CEb, normalTP_CEb, u_D, b_D\n",
    "    \n",
    "def CornerDatasetGenerator(data, nodesIE, distanceBE, nodesD, batch_size, n_batches, elSizeMin=0.04, elSizeMax=0.07):\n",
    "    for i in range(batch_size*n_batches):\n",
    "        nodesTPC, elementsTPC, uTP_CEi, bTP_CEi, uTP_CEb, normalTP_CEb, u_D, b_D =  GetRandomTrainingPatchCorner(data, nodesIE, distanceBE, nodesD, elSizeMin, elSizeMax)\n",
    "        nodesTPC = tf.convert_to_tensor(nodesTPC, dtype=tf.float32)\n",
    "        elementsTPC = tf.convert_to_tensor(elementsTPC, dtype=tf.int32)\n",
    "        uTP_CEi = tf.convert_to_tensor(uTP_CEi, dtype=tf.float32)\n",
    "        bTP_CEi = tf.convert_to_tensor(bTP_CEi, dtype=tf.float32)\n",
    "        uTP_CEb = tf.convert_to_tensor(uTP_CEb, dtype=tf.float32)\n",
    "        normalTP_CEb = tf.convert_to_tensor(normalTP_CEb, dtype=tf.float32)\n",
    "        u_D = tf.convert_to_tensor(u_D, dtype=tf.float32)\n",
    "        b_D = tf.convert_to_tensor(b_D, dtype=tf.float32)\n",
    "        yield nodesTPC, elementsTPC, uTP_CEi, bTP_CEi, uTP_CEb, normalTP_CEb, u_D, b_D\n",
    "\n",
    "def GetCornerDataset(data, nodesIE, distanceBE, nodesD, batch_size, n_batches, elSizeMin=0.04, elSizeMax=0.07):\n",
    "    d_IE = data[0]['interpSE'].nDims\n",
    "    d_BE = data[0]['interpBS'][0].nDims\n",
    "    d_D = data[0]['interpSD'].nDims\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: CornerDatasetGenerator(data, nodesIE, distanceBE, nodesD, batch_size, n_batches, elSizeMin=elSizeMin, elSizeMax=elSizeMax),\n",
    "        output_types=(tf.float32, tf.int32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32),\n",
    "        output_shapes=(tf.TensorShape([7,2]), tf.TensorShape([6,3]), \n",
    "                       tf.TensorShape([4,nodesIE.shape[0],d_IE]), tf.TensorShape([4,nodesIE.shape[0],1]), \n",
    "                       tf.TensorShape([3,distanceBE.shape[0],d_BE]), tf.TensorShape([3,distanceBE.shape[0],2]), \n",
    "                       tf.TensorShape([nodesD.shape[0],d_D]), tf.TensorShape([nodesD.shape[0],1])))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def GetCornerDatasetFixed(data, nodesIE, distanceBE, nodesD, batch_size, n_batches, elSizeMin=0.04, elSizeMax=0.07,sRand=0.1):\n",
    "    d_IE = data[0]['interpSE'].nDims\n",
    "    d_BE = data[0]['interpBS'][0].spline.c.shape[-1]\n",
    "    d_D = data[0]['interpSD'].nDims\n",
    "\n",
    "    dataset_tesor_nodesTPC = np.zeros((batch_size*n_batches, 7, 2),dtype=np.float32)\n",
    "    dataset_tesor_elementsTPC = np.zeros((batch_size*n_batches, 6, 3),dtype=np.int32)\n",
    "    dataset_tesor_uTP_CEi = np.zeros((batch_size*n_batches, 4, nodesIE.shape[0], d_IE),dtype=np.float32)\n",
    "    dataset_tesor_bTP_CEi = np.zeros((batch_size*n_batches, 4, nodesIE.shape[0], 1),dtype=np.float32)\n",
    "    dataset_tesor_uTP_CEb = np.zeros((batch_size*n_batches, 3, distanceBE.shape[0], d_BE),dtype=np.float32)\n",
    "    dataset_tesor_normalTP_CEb = np.zeros((batch_size*n_batches, 3, distanceBE.shape[0], 2),dtype=np.float32)\n",
    "    dataset_tesor_u_D = np.zeros((batch_size*n_batches, nodesD.shape[0], d_D),dtype=np.float32)\n",
    "    dataset_tesor_b_D = np.zeros((batch_size*n_batches, nodesD.shape[0], 1),dtype=np.float32)\n",
    "    for i in tqdm(range(batch_size*n_batches)):\n",
    "        nodesTPC, elementsTPC, uTP_CEi, bTP_CEi, uTP_CEb, normalTP_CEb, u_D, b_D =  GetRandomTrainingPatchCorner(data, nodesIE, distanceBE, nodesD, elSizeMin, elSizeMax,sRand=sRand)\n",
    "        dataset_tesor_nodesTPC[i] = nodesTPC\n",
    "        dataset_tesor_elementsTPC[i] = elementsTPC\n",
    "        dataset_tesor_uTP_CEi[i] = uTP_CEi\n",
    "        dataset_tesor_bTP_CEi[i] = bTP_CEi\n",
    "        dataset_tesor_uTP_CEb[i] = uTP_CEb\n",
    "        dataset_tesor_normalTP_CEb[i] = normalTP_CEb\n",
    "        dataset_tesor_u_D[i] = u_D\n",
    "        dataset_tesor_b_D[i] = b_D\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataset_tesor_nodesTPC, dataset_tesor_elementsTPC,\n",
    "                                                    dataset_tesor_uTP_CEi, dataset_tesor_bTP_CEi,\n",
    "                                                    dataset_tesor_uTP_CEb, dataset_tesor_normalTP_CEb,\n",
    "                                                    dataset_tesor_u_D, dataset_tesor_b_D))\n",
    "    \n",
    "    dataset = dataset.shuffle(batch_size*n_batches).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# normalisation test code\n",
    "# nodesTP, elementsTP = GenerateTriangTrainMeshInter()\n",
    "# df = pd.DataFrame(nodesTP, columns=['x', 'y'])\n",
    "# df['u'] = .5*df['x'] + -0.3*df['y']+1+np.random.normal(0, 0.0, len(df))\n",
    "# df['1'] = 1\n",
    "\n",
    "# X = df[['1', 'x', 'y']].values\n",
    "# f = df['u'].values\n",
    "# theta = np.linalg.solve(X.T @ X, X.T @ f)\n",
    "# print(theta)\n",
    "# fig = px.scatter_3d(df, x='x', y='y', z='u')\n",
    "\n",
    "# xx,yy = np.meshgrid(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100))\n",
    "# zz = theta[0] + theta[1]*xx + theta[2]*yy\n",
    "\n",
    "# fig.add_trace(go.Surface(x=xx, y=yy, z=zz, opacity=0.5, name='Fitted Plane'))\n",
    "# fig.show()\n",
    "\n",
    "file_path = r'TrainingData/TurbulentFlowData/2400.am'\n",
    "data = loadData(file_path, np.arange(-50,0,5))\n",
    "data_train = data[:1]\n",
    "nodesIE = GetRadialEncoderInputMask(10,8,0.5,0.05)\n",
    "nodesD = GetRadialEncoderInputMask(0,1,0.5,0.05)\n",
    "\n",
    "dataset = GetInteriorDatasetFixed(data_train, nodesIE, nodesD, 10, 10, elSizeMin=0.1, elSizeMax=0.2)\n",
    "# dataset_b = GetBoundaryDatasetFixed(data_train, nodesIE, distanceBE, nodesD, batch_size=batch_size[1], n_batches=n_batches, elSizeMin=0.1, elSizeMax=0.2)\n",
    "# dataset_c = GetCornerDatasetFixed(data, nodesIE, distanceBE, nodesD, 10, 10, elSizeMin=0.21, elSizeMax=0.22)\n",
    "# gen = BoundaryDatasetGenerator(data[:1], nodesIE, distanceBE, nodesD, batch_size[1], n_batches, elSizeMin=0.1, elSizeMax=0.2)\n",
    "# i = 0\n",
    "# for i_d in gen:\n",
    "#     if i == 1000:\n",
    "#         break\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEncoder(N_interiorEncoder,n_latent,layers,d_input=1,activation='tanh'):\n",
    "    inputU = Input((N_interiorEncoder,d_input))\n",
    "    inputB = Input((N_interiorEncoder,1))\n",
    "    x = Concatenate(axis = 1)([Flatten()(inputU),Flatten()(inputB)])\n",
    "    for i in range(len(layers)):\n",
    "        x = Dense(layers[i],activation=activation)(x)\n",
    "    latent = Dense(n_latent)(x)\n",
    "    encoder = Model([inputU,inputB],latent,name='encoder')\n",
    "    return encoder\n",
    "\n",
    "def GetEncoderBoundary(N_boundarEncoder,n_latent,layers,d_input=1,activation='tanh'):\n",
    "    inputU = Input((N_boundarEncoder,d_input))\n",
    "    inputB = Input((N_boundarEncoder,2))\n",
    "    x = Concatenate(axis = 1)([Flatten()(inputU),Flatten()(inputB)])\n",
    "    for i in range(len(layers)):\n",
    "        x = Dense(layers[i],activation=activation)(x)\n",
    "    latent = Dense(n_latent)(x)\n",
    "    encoderB = Model([inputU,inputB],latent,name='encoderB')\n",
    "    return encoderB\n",
    "\n",
    "def GetDecoder(N_decoderOut,n_latent,layers,d_out=1,activation='tanh'):\n",
    "    input = Input((n_latent))\n",
    "    x = input\n",
    "    for i in range(len(layers)-1,-1,-1):\n",
    "        x = Dense(layers[i],activation=activation)(x)\n",
    "    output = Dense(N_decoderOut*d_out)(x)\n",
    "    output = Reshape((N_decoderOut,d_out))(output)\n",
    "    decoder = Model(input,output,name='decoder')\n",
    "    return decoder\n",
    "\n",
    "def GetModels(hyperParams):\n",
    "    # Get the encoder, boundary encoder and decoder models based on the hyperparameters\n",
    "    N_interiorEncoder = hyperParams['N_interiorEncoder']\n",
    "    d_interiorEncoder = hyperParams['d_interiorEncoder']\n",
    "    layers_interiorEncoder = hyperParams['layers_interiorEncoder']\n",
    "    N_boundarEncoder = hyperParams['N_boundarEncoder']\n",
    "    d_boundarEncoder = hyperParams['d_boundarEncoder']\n",
    "    layers_boundarEncoder = hyperParams['layers_boundarEncoder']\n",
    "    N_decoderOut = hyperParams['N_decoderOut']\n",
    "    d_decoderOut = hyperParams['d_decoderOut']\n",
    "    layers_decoder = hyperParams['layers_decoder']\n",
    "    n_latent = hyperParams['n_latent']\n",
    "    activation = hyperParams['activation']\n",
    "    encoder = GetEncoder(N_interiorEncoder,n_latent,layers_interiorEncoder,d_interiorEncoder,activation)\n",
    "    encoderB = GetEncoderBoundary(N_boundarEncoder,n_latent,layers_boundarEncoder,d_boundarEncoder,activation)\n",
    "    decoder = GetDecoder(N_decoderOut,n_latent,layers_decoder,d_decoderOut,activation)\n",
    "    return encoder,encoderB,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetK_el_triang(A,nodes):\n",
    "    r = int(A.shape[0]/2)\n",
    "    b = np.roll(nodes[:,1],1) - np.roll(nodes[:,1],2)\n",
    "    c = np.roll(nodes[:,0],2) - np.roll(nodes[:,0],1)\n",
    "    Area = np.abs(np.dot(nodes[:,0],b))/2\n",
    "    B = np.concatenate([\n",
    "        np.concatenate([b[i]*np.eye(r) for i in range(3)],1),\n",
    "        np.concatenate([c[i]*np.eye(r) for i in range(3)],1)\n",
    "    ],0)/(2*Area)\n",
    "    return np.dot(np.dot(B.T,A),B)*Area\n",
    "\n",
    "\n",
    "def SolveFEM(nodes, elements, boundaryNodes, l_BC, internalNodes, r, A, A_nl=False, l=None):\n",
    "    if l is None:\n",
    "        l = np.zeros((nodes.shape[0], r))\n",
    "    if not A_nl:\n",
    "        A_l = A\n",
    "\n",
    "    # Assemble the global stiffness matrix\n",
    "    K = np.zeros((nodes.shape[0]*r, nodes.shape[0]*r))\n",
    "    for el in elements:\n",
    "        el_idx = [[r*k+j for j in range(r)] for k in el]\n",
    "        el_idx = np.concatenate(el_idx)\n",
    "        nodes_el = tf.gather(nodes, indices=el)\n",
    "        X_idx,Y_idx = np.meshgrid(el_idx,el_idx)\n",
    "        if A_nl:\n",
    "            A_l = A(l[el_idx])\n",
    "        # print(A_l)\n",
    "        K_el = GetK_el_triang(A_l,nodes_el)\n",
    "        K[Y_idx,X_idx] += K_el\n",
    "\n",
    "\n",
    "    bc_idx = [[r*i+j for j in range(r)] for i in boundaryNodes]\n",
    "    bc_idx = np.concatenate(bc_idx)\n",
    "    internal_idx = [[r*i+j for j in range(r)] for i in internalNodes]\n",
    "    internal_idx = np.concatenate(internal_idx)\n",
    "\n",
    "    f = - (K[:,bc_idx] @ l_BC.flatten().reshape(-1,1))\n",
    "\n",
    "    K_BC = K[internal_idx,:][:,internal_idx]\n",
    "    f = f[internal_idx]\n",
    "\n",
    "    # Solve the system\n",
    "    l_internal = np.linalg.solve(K_BC, f)\n",
    "    n_CDOF = int(l_internal.shape[0]/r)\n",
    "    l_internal = l_internal.reshape(n_CDOF, r)\n",
    "\n",
    "    l[internalNodes,:] = l_internal\n",
    "    l[boundaryNodes,:] = l_BC.reshape(-1,r)\n",
    "    return l\n",
    "\n",
    "def SINNsPredict(data,encoderB,decoder,B,r,distanceBE):\n",
    "    nodes = data['nodes']\n",
    "    elements = data['elements']\n",
    "    boundaryNodes = np.concatenate([idxCurve[:-1] for idxCurve in data['idxCurves']])\n",
    "    internalNodes = np.setdiff1d(np.unique(elements.flatten()),boundaryNodes)\n",
    "    u_D = data['solution'][...,:2]\n",
    "    # distance= data['distanceCurves']\n",
    "    distance = [d[:-1] for d in data['distanceCurves']]\n",
    "    interpBS = data['interpBS']\n",
    "    interpBN = data['interpBN']\n",
    "    nd_BE = data['interpBS'][0].spline.c.shape[-1]\n",
    "\n",
    "    dBE_S = [(d.reshape(-1,1) + distanceBE.reshape(1,-1)) for d in distance]\n",
    "    u_BE = [interpBS[i](d.flatten()).reshape(d.shape[0],d.shape[1],nd_BE) for i,d in enumerate(dBE_S)]\n",
    "    u_BE = np.concatenate(u_BE,0)\n",
    "    normal_BE = [interpBN[i](d.flatten()).reshape(d.shape[0],d.shape[1],2) for i,d in enumerate(dBE_S)]\n",
    "    normal_BE = np.concatenate(normal_BE,0)\n",
    "\n",
    "    lB = encoderB([u_BE,normal_BE])\n",
    "\n",
    "\n",
    "    l_model = SolveFEM(nodes, elements, boundaryNodes, lB.numpy(), internalNodes, r, GetA(B).numpy())\n",
    "    l_modelI = l_model[internalNodes]\n",
    "    u_modelI = decoder(l_modelI)\n",
    "    u_model = u_D.copy()\n",
    "    u_model[internalNodes] = u_modelI[:,0,:].numpy()\n",
    "    return u_model, l_model\n",
    "\n",
    "def EncodeData(data,encoder,encoderB, nodesIE, distanceBE):\n",
    "    nodes = data['nodes']\n",
    "    boundaryNodes = np.concatenate([idxCurve[:-1] for idxCurve in data['idxCurves']])\n",
    "    internalNodes = np.setdiff1d(np.arange(nodes.shape[0]),boundaryNodes)\n",
    "    nodesI = nodes[internalNodes]\n",
    "    distance = [d[:-1] for d in data['distanceCurves']]\n",
    "    interpSE = data['interpSE']\n",
    "    interpBS = data['interpBS']\n",
    "    interpBN = data['interpBN']\n",
    "    isInDomainF = data['isInDomainF']\n",
    "    # nd_IE = data['interpSE'].nDims\n",
    "    nd_BE = data['interpBS'][0].spline.c.shape[-1]\n",
    "    # nd_D = data['interpSD'].nDims\n",
    "\n",
    "    dBE_S = [(d.reshape(-1,1) + distanceBE.reshape(1,-1)) for d in distance]\n",
    "    u_BE = [interpBS[i](d.flatten()).reshape(d.shape[0],d.shape[1],nd_BE) for i,d in enumerate(dBE_S)]\n",
    "    u_BE = np.concatenate(u_BE,0)\n",
    "    normal_BE = [interpBN[i](d.flatten()).reshape(d.shape[0],d.shape[1],2) for i,d in enumerate(dBE_S)]\n",
    "    normal_BE = np.concatenate(normal_BE,0)\n",
    "\n",
    "    lB = encoderB([u_BE,normal_BE])\n",
    "\n",
    "    nodesI_IE = GetEncoderInputTP(nodesI,nodesIE)\n",
    "    uI_IE = interpSE(nodesI_IE.reshape(-1,2)).reshape(nodesI_IE.shape[0],nodesI_IE.shape[1],-1)\n",
    "    bI_IE = isInDomainF(nodesI_IE.reshape(-1,2)).reshape(nodesI_IE.shape[0],nodesI_IE.shape[1],-1)\n",
    "    uI_IE[~bI_IE[:,:,0]] = 0.0\n",
    "\n",
    "    lI = encoder([uI_IE,bI_IE])\n",
    "\n",
    "    l = np.zeros((nodes.shape[0],lI.shape[-1]))\n",
    "    l[internalNodes] = lI\n",
    "    l[boundaryNodes] = lB\n",
    "    return l\n",
    "\n",
    "def PlotFEMsolution(nodes, elements,l):\n",
    "    if elements.shape[1] == 4:\n",
    "        # Convert quadrlateral mesh to triangular mesh\n",
    "        elements = np.concatenate([elements[:,:3],elements[:,1:]],0)\n",
    "\n",
    "    # Create a Triangulation object\n",
    "    triangulation = tri.Triangulation(nodes[:, 0], nodes[:, 1], elements)\n",
    "\n",
    "    # Plotting\n",
    "    r = l.shape[1]\n",
    "    plt.figure(figsize=(6*r,5))\n",
    "    for i in range(r):\n",
    "        plt.subplot(1,r,i+1)\n",
    "        plt.tricontourf(triangulation, l[:,i],10)\n",
    "        plt.colorbar()\n",
    "        # plt.scatter(nodes[:,0],nodes[:,1],s=1,c='k')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "    return\n",
    "\n",
    "def SINNsEvalErr(data,encoderB,decoder,B,r,distanceBE):\n",
    "    u_model,_  = SINNsPredict(data,encoderB,decoder,B,r,distanceBE)\n",
    "    u = data['solution'][...,:2]\n",
    "    return np.mean(np.square(u_model-u))\n",
    "\n",
    "def GetK_tf(nodes_el, A, r):\n",
    "    # Get the stiffness matrix for a triangular elements, function is vectorized so that a batch of elements can be processed at once\n",
    "    #   nodes_el - [N,3,2] tensor which containes the x,y positions of the 3 nodes of N triangles\n",
    "    #   A - [2*r,2*r] tensor which defines the linear PDE coefficients\n",
    "    #   r - number of latent variables\n",
    "\n",
    "    # calculate the triangle area and the B matrix which is a derivative of the shape functions (B = div(N))\n",
    "    b = (tf.roll(nodes_el[:,:,1],1,axis=1) - tf.roll(nodes_el[:,:,1],2,axis=1))\n",
    "    b = tf.reshape(b,(-1,3,1))\n",
    "    c = (tf.roll(nodes_el[:,:,0],2,axis=1) - tf.roll(nodes_el[:,:,0],1,axis=1))\n",
    "    c = tf.reshape(c,(-1,3,1))\n",
    "    Area = tf.abs(tf.matmul(tf.reshape(nodes_el[:,:,0],(-1,1,3)),b))/2\n",
    "    I = tf.reshape(tf.eye(r),(1,r,r))\n",
    "    B = tf.concat([\n",
    "        tf.concat([b[:,0:1]*I, b[:,1:2]*I, b[:,2:3]*I],-1),\n",
    "        tf.concat([c[:,0:1]*I, c[:,1:2]*I, c[:,2:3]*I],-1)\n",
    "    ],-2)/(2*Area)\n",
    "    B_T = tf.transpose(B,(0,2,1))\n",
    "    return tf.matmul(tf.matmul(B_T,A),B)*Area # calculates the stiffness matrix\n",
    "\n",
    "def PredictLatentC(nodes_el, l_el_b, A, r):\n",
    "    # Predicts the latent value of a cetral node of a training patch mesh made up of 6 triangular elements in the shape of hexagon\n",
    "    # Theoretically could be extended to more general training patch mesh but has not been done yet\n",
    "    #   nodes_el - [N,6,3,2] tensor which containes the x,y positions of the 3 nodes of 6 triangle elements of N training patches\n",
    "    #   l_el_b - [N,6,2,r] tensor which containes the latent values of the boundary nodes of the 6 triangle elements of N training patches\n",
    "    #   A - [2*r,2*r] tensor which defines the linear PDE coefficients\n",
    "    #   r - number of latent variables\n",
    "    l_el_b = tf.concat([l_el_b[...,0,:],l_el_b[...,1,:]],-1)[...,tf.newaxis]\n",
    "    nodes_el_flat = tf.reshape(nodes_el,(-1,nodes_el.shape[-2],2))\n",
    "    K_el_flat = GetK_tf(nodes_el_flat, A, r)\n",
    "    K_el = tf.reshape(K_el_flat,(nodes_el.shape[0],nodes_el.shape[1],K_el_flat.shape[-2],K_el_flat.shape[-1]))\n",
    "    K = tf.reduce_sum(K_el[:,:,:r,:r],1)\n",
    "    b = -K_el[:,:,:r,r:]@l_el_b\n",
    "    b = tf.reduce_sum(b,1)\n",
    "    l_c = tf.linalg.solve(K,b)\n",
    "    return l_c[...,0]\n",
    "\n",
    "def GetA(B):\n",
    "    A = tf.matmul(B,B,transpose_a=True)\n",
    "    return A\n",
    "\n",
    "# def LossFunc(u_D,u_D_pred,b_D,l_c,l_c_pred,A,fd_l_weight,ellicpit_weight):\n",
    "#     r_loss = tf.reduce_mean(tf.square((u_D-u_D_pred)*b_D),axis=(1,2))\n",
    "#     fd_l_loss = tf.reduce_mean(tf.square(l_c-l_c_pred),axis=(1))\n",
    "#     elliptic_loss = -tf.math.log(tf.linalg.det(A))\n",
    "#     loss = r_loss + fd_l_weight*fd_l_loss + ellicpit_weight*elliptic_loss\n",
    "#     return loss, (tf.reduce_mean(loss),tf.reduce_mean(r_loss),tf.reduce_mean(fd_l_loss),tf.reduce_mean(elliptic_loss))\n",
    "\n",
    "def LossFunc2(u_D,u_D_pred,b_D,l_enc,l_c_pred,A,fd_l_weight,ellicpit_weight):\n",
    "    r_loss = tf.reduce_mean(tf.square((u_D-u_D_pred)*b_D),axis=(1,2))\n",
    "    # fd_l_loss = tf.reduce_mean(tf.square(l_enc[:,0,:]-l_c_pred)/tf.math.reduce_variance(l_enc,axis=1),axis=(1))\n",
    "    fd_l_loss = tf.reduce_mean(tf.square(l_enc[:,0,:]-l_c_pred)/tf.math.pow(tf.math.reduce_variance(l_enc,axis=1),0.8),axis=(1))\n",
    "    # fd_l_loss = tf.reduce_mean(tf.square(l_enc[:,0,:]-l_c_pred)/tf.math.reduce_std(l_enc,axis=1),axis=(1))\n",
    "    elliptic_loss = -tf.math.log(tf.linalg.det(A))\n",
    "    loss = r_loss + fd_l_weight*fd_l_loss + ellicpit_weight*elliptic_loss\n",
    "    return loss, (tf.reduce_mean(loss),tf.reduce_mean(r_loss),tf.reduce_mean(fd_l_loss),tf.reduce_mean(elliptic_loss))\n",
    "\n",
    "def InteriorForwardPass(encoder, decoder, A, r, nodesTP, elementsTP, uTP_IE, bTP_IE, u_D, b_D, fd_l_weight, elliptc_weight):\n",
    "    uTP_IE_reshaped = tf.reshape(uTP_IE,(uTP_IE.shape[0]*uTP_IE.shape[1],uTP_IE.shape[2],uTP_IE.shape[3]))\n",
    "    bTP_IE_reshaped = tf.reshape(bTP_IE,(bTP_IE.shape[0]*bTP_IE.shape[1],bTP_IE.shape[2]))\n",
    "    l_reshaped = encoder([uTP_IE_reshaped,bTP_IE_reshaped])\n",
    "    l = tf.reshape(l_reshaped,(uTP_IE.shape[0],uTP_IE.shape[1],l_reshaped.shape[-1]))\n",
    "    l_el_b = tf.gather(l,elementsTP[0,:,1:], axis=1)\n",
    "    nodes_el = tf.gather(nodesTP,elementsTP[0], axis=1)\n",
    "    l_c_pred = PredictLatentC(nodes_el,l_el_b,A,r)\n",
    "    u_D_pred = decoder(l_c_pred)\n",
    "    # loss, logloss = LossFunc(u_D,u_D_pred,b_D,l[:,0],l_c_pred,A,fd_l_weight,elliptc_weight)\n",
    "    loss, logloss = LossFunc2(u_D,u_D_pred,b_D,l,l_c_pred,A,fd_l_weight,elliptc_weight)\n",
    "    return loss, logloss\n",
    "\n",
    "\n",
    "def BoundaryForwardPass(encoder, encoderB, decoder, A, r, nodesTPB, elementsTPB, uTP_BEi, bTP_BEi, uTP_BEb, normalTP_BEb, u_D, b_D, fd_l_weight, elliptc_weight):\n",
    "    uTP_BEi_reshaped = tf.reshape(uTP_BEi,(uTP_BEi.shape[0]*uTP_BEi.shape[1],*uTP_BEi.shape[2:]))\n",
    "    bTP_BEi_reshaped = tf.reshape(bTP_BEi,(bTP_BEi.shape[0]*bTP_BEi.shape[1],*bTP_BEi.shape[2:]))\n",
    "    l_i_reshaped = encoder([uTP_BEi_reshaped,bTP_BEi_reshaped])\n",
    "    l_i = tf.reshape(l_i_reshaped,(uTP_BEi.shape[0],uTP_BEi.shape[1],l_i_reshaped.shape[-1]))\n",
    "    uTP_BEb_reshaped = tf.reshape(uTP_BEb,(uTP_BEb.shape[0]*uTP_BEb.shape[1],*uTP_BEb.shape[2:]))\n",
    "    normalTP_BEb_reshaped = tf.reshape(normalTP_BEb,(normalTP_BEb.shape[0]*normalTP_BEb.shape[1],*normalTP_BEb.shape[2:]))\n",
    "    l_b_reshaped = encoderB([uTP_BEb_reshaped,normalTP_BEb_reshaped])\n",
    "    l_b = tf.reshape(l_b_reshaped,(uTP_BEb.shape[0],uTP_BEb.shape[1],l_b_reshaped.shape[-1]))\n",
    "    l = tf.concat([l_i[:,0:1],l_b,l_i[:,1:]],-2)\n",
    "    l_el_b = tf.gather(l,elementsTPB[0,:,1:], axis=1)\n",
    "    nodes_el = tf.gather(nodesTPB,elementsTPB[0], axis=1)\n",
    "    l_c_pred = PredictLatentC(nodes_el,l_el_b,A,r)\n",
    "    u_D_pred = decoder(l_c_pred)\n",
    "    # loss, logloss = LossFunc(u_D,u_D_pred,b_D,l[:,0],l_c_pred,A,fd_l_weight,elliptc_weight)\n",
    "    loss, logloss = LossFunc2(u_D,u_D_pred,b_D,l,l_c_pred,A,fd_l_weight,elliptc_weight)\n",
    "    return loss, logloss\n",
    "\n",
    "def CornerForwardPass(encoder, encoderB, decoder, A, r, nodesTPC, elementsTPC, uTP_CEi, bTP_CEi, uTP_CEb, normalTP_CEb, u_D, b_D, fd_l_weight, elliptc_weight):\n",
    "    uTP_CEi_reshaped = tf.reshape(uTP_CEi,(uTP_CEi.shape[0]*uTP_CEi.shape[1],*uTP_CEi.shape[2:]))\n",
    "    bTP_CEi_reshaped = tf.reshape(bTP_CEi,(bTP_CEi.shape[0]*bTP_CEi.shape[1],*bTP_CEi.shape[2:]))\n",
    "    l_i_reshaped = encoder([uTP_CEi_reshaped,bTP_CEi_reshaped])\n",
    "    l_i = tf.reshape(l_i_reshaped,(uTP_CEi.shape[0],uTP_CEi.shape[1],l_i_reshaped.shape[-1]))\n",
    "    uTP_CEb_reshaped = tf.reshape(uTP_CEb,(uTP_CEb.shape[0]*uTP_CEb.shape[1],*uTP_CEb.shape[2:]))\n",
    "    normalTP_CEb_reshaped = tf.reshape(normalTP_CEb,(normalTP_CEb.shape[0]*normalTP_CEb.shape[1],*normalTP_CEb.shape[2:]))\n",
    "    l_b_reshaped = encoderB([uTP_CEb_reshaped,normalTP_CEb_reshaped])\n",
    "    l_b = tf.reshape(l_b_reshaped,(uTP_CEb.shape[0],uTP_CEb.shape[1],l_b_reshaped.shape[-1]))\n",
    "    l = tf.concat([l_i[:,0:1],l_b[:,:2],l_i[:,1:],l_b[:,2:3]],-2)\n",
    "    l_el_b = tf.gather(l,elementsTPC[0,:,1:], axis=1)\n",
    "    nodes_el = tf.gather(nodesTPC,elementsTPC[0], axis=1)\n",
    "    l_c_pred = PredictLatentC(nodes_el,l_el_b,A,r)\n",
    "    u_D_pred = decoder(l_c_pred)\n",
    "    # loss, logloss = LossFunc(u_D,u_D_pred,b_D,l[:,0],l_c_pred,A,fd_l_weight,elliptc_weight)\n",
    "    loss, logloss = LossFunc2(u_D,u_D_pred,b_D,l,l_c_pred,A,fd_l_weight,elliptc_weight)\n",
    "    return loss, logloss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def TrainStep(encoder, encoderB, decoder, B, optimizer, batchI, batchB, batchC, r, fd_l_weight, elliptc_weight):\n",
    "\n",
    "    with tf.GradientTape() as enc_tape, tf.GradientTape() as encB_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as b_tape:\n",
    "        A = GetA(B)\n",
    "        lossI, loglossI = InteriorForwardPass(encoder, decoder, A, r, *batchI, fd_l_weight, elliptc_weight)\n",
    "        lossB, loglossB = BoundaryForwardPass(encoder, encoderB, decoder, A, r, *batchB, fd_l_weight, elliptc_weight)\n",
    "        lossC, loglossC = CornerForwardPass(encoder, encoderB, decoder, A, r, *batchC, fd_l_weight, elliptc_weight)\n",
    "        loss = tf.concat([lossI,lossB,lossC],0)\n",
    "\n",
    "    grads_enc = enc_tape.gradient(loss,encoder.trainable_variables)\n",
    "    grads_encB = encB_tape.gradient(loss,encoderB.trainable_variables)\n",
    "    grads_dec = dec_tape.gradient(loss,decoder.trainable_variables)\n",
    "    grads_b = b_tape.gradient(loss,B)\n",
    "    optimizer.apply_gradients(zip(grads_enc,encoder.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(grads_dec,decoder.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(grads_encB,encoderB.trainable_variables))\n",
    "    optimizer.apply_gradients(zip([grads_b],[B]))\n",
    "\n",
    "    return loglossI+loglossB+loglossC\n",
    "\n",
    "def TrainModel(encoder, encoderB, decoder, B, optimizer, dataset, dataset_b, dataset_c, r, epochs, n_batches, hyperParams, history=None, data_test=None, data_train=None):\n",
    "    fd_l_weight = hyperParams['fd_l_weight']\n",
    "    elliptc_weight = hyperParams['elliptc_weight']\n",
    "    distanceBE = np.array(hyperParams['distanceBE'])\n",
    "    if history is None:\n",
    "        history = {'loss':[], 'loss_i':[], 'loss_b':[], 'loss_c': [],'r_loss_i':[],'r_loss_b':[],'r_loss_c':[],'fd_l_loss_i':[],'fd_l_loss_b':[],'fd_l_loss_c':[],'elliptic_loss':[],'err_train':[],'err_test':[],'err_epoch':[]}\n",
    "    history_epoch = np.zeros((n_batches,12))\n",
    "    for epoch in range(epochs):\n",
    "        for i, (batchI, batchB, batchC) in enumerate(zip(dataset,dataset_b,dataset_c)):\n",
    "            logloss = TrainStep(encoder, encoderB, decoder, B, optimizer, batchI, batchB, batchC, r, fd_l_weight, elliptc_weight)\n",
    "            history_epoch[i,:] = logloss\n",
    "            print(f'epoch: {epoch+1}/{epochs}; batch: {i+1}/{n_batches}; loss: {history_epoch[:,0].mean():.4g}', end='\\r')\n",
    "        \n",
    "        logloss = history_epoch.mean(0)\n",
    "        history['loss'].append((logloss[0]+logloss[4]+logloss[8])/3)\n",
    "        history['loss_i'].append(logloss[0])\n",
    "        history['loss_b'].append(logloss[4])\n",
    "        history['loss_c'].append(logloss[8])\n",
    "        history['r_loss_i'].append(logloss[1])\n",
    "        history['r_loss_b'].append(logloss[5])\n",
    "        history['r_loss_c'].append(logloss[9])\n",
    "        history['fd_l_loss_i'].append(logloss[2])\n",
    "        history['fd_l_loss_b'].append(logloss[6])\n",
    "        history['fd_l_loss_c'].append(logloss[10])\n",
    "        history['elliptic_loss'].append(logloss[3])\n",
    "        print(f'epoch: {epoch+1}/{epochs}; batch: {i+1}/{n_batches}; loss: {history_epoch[:,0].mean():.4g}', end='')\n",
    "\n",
    "        if (data_test is not None) and ((epoch+1)%5 == 0):\n",
    "            err_arr = np.zeros((len(data_test)))\n",
    "            for i in range(len(data_test)):\n",
    "                err_arr[i] = SINNsEvalErr(data_test[i],encoderB,decoder,B,r,distanceBE)\n",
    "            history['err_test'].append(err_arr.mean())\n",
    "            history['err_epoch'].append(len(history['loss'])-1)\n",
    "            print(f'; err_test: {err_arr.mean():.4g}', end='')\n",
    "            if (data_train is not None):\n",
    "                err_arr_train = np.zeros((len(data_train)))\n",
    "                for i in range(len(data_train)):\n",
    "                    err_arr_train[i] = SINNsEvalErr(data_train[i],encoderB,decoder,B,r,distanceBE)\n",
    "                history['err_train'].append(err_arr_train.mean())\n",
    "                print(f'; err_train: {err_arr_train.mean():.4g}', end='')\n",
    "            print()\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "    return history\n",
    "\n",
    "def Save(modelFolder,encoder,encoderB,decoder,P,history,hyperParams):\n",
    "    # Saves the all SINNs models, the training history and the hyper parameters\n",
    "    if not os.path.exists(modelFolder):\n",
    "        # Create folder for the model if it does not exist\n",
    "        os.makedirs(modelFolder)      \n",
    "    \n",
    "    # Save models within the folder\n",
    "    encoder.save(modelFolder+\"/encoder.keras\")\n",
    "    encoderB.save(modelFolder+\"/encoderB.keras\")\n",
    "    decoder.save(modelFolder+\"/decoder.keras\")\n",
    "    np.save(modelFolder+\"/P.npy\",P.numpy())\n",
    "    with open( modelFolder+'/history.json', 'w') as fp:\n",
    "        json.dump(history, fp)\n",
    "    with open( modelFolder+'/hyperParams.json', 'w') as fp:\n",
    "        json.dump(hyperParams, fp)\n",
    "\n",
    "def LoadModels(modelFolder):\n",
    "    # Load SINNs models, the training history and the hyper parameters\n",
    "    encoder = tf.keras.models.load_model(modelFolder+\"/encoder.keras\")\n",
    "    encoderB = tf.keras.models.load_model(modelFolder+\"/encoderB.keras\")\n",
    "    decoder = tf.keras.models.load_model(modelFolder+\"/decoder.keras\")\n",
    "    P = np.load(modelFolder+\"/P.npy\")\n",
    "    P = tf.Variable(tf.constant(P,tf.float32))\n",
    "    with open(modelFolder+'/history.json', 'r') as fp:\n",
    "        history = json.load(fp)\n",
    "    with open(modelFolder+'/hyperParams.json', 'r') as fp:\n",
    "        hyperParams = json.load(fp)\n",
    "    return encoder,encoderB,decoder,P,history,hyperParams\n",
    "\n",
    "def polygon_area(points):\n",
    "    n = len(points)\n",
    "    if n < 3:\n",
    "        return 0  # Not a polygon\n",
    "    \n",
    "    area = 0\n",
    "    for i in range(n-1):\n",
    "        j = (i + 1)\n",
    "        x_i, y_i = points[i]\n",
    "        x_j, y_j = points[j]\n",
    "        area += x_i * y_j - x_j * y_i\n",
    "\n",
    "    return abs(area) / 2\n",
    "\n",
    "filePath = 'TrainingData/LowReFlowDataProcessed/dataLowReFLow2.json'\n",
    "derL = 0.05\n",
    "data = loadData(filePath,derL)\n",
    "\n",
    "areas = np.array([polygon_area(data_i['nodesCurves'][1]) for data_i in data])\n",
    "mask = (areas>2) & (areas<3)\n",
    "idx_sub = np.where((areas>2) & (areas<3))[0]\n",
    "data = [data[i] for i in idx_sub]\n",
    "data_train = data[:-10]\n",
    "data_test = data[-10:]\n",
    "data_test_remesh = [RemeshData(data_i,0.2) for data_i in data_test[:10]]\n",
    "data_train_remesh = [RemeshData(data_i,0.2) for data_i in data_train[:10]]\n",
    "\n",
    "\n",
    "batch_size = [128,64,16]\n",
    "# batch_size = [64,32,8]\n",
    "# n_batches = 300\n",
    "n_batches = 1_000\n",
    "n_epochs = 180\n",
    "# layers = [32,32,32]\n",
    "layers = [256,256,256,256]\n",
    "deltaElSize = 0.05\n",
    "spreadElSize = 0.01\n",
    "baseElSize = 0.2\n",
    "hyperParams = {'N_interiorEncoder': None, 'd_interiorEncoder': 3, 'layers_interiorEncoder': layers,\n",
    "               'N_boundarEncoder': None, 'd_boundarEncoder': 3+3*bool(derL), 'layers_boundarEncoder': layers,\n",
    "               'N_decoderOut': None, 'd_decoderOut': 2, 'layers_decoder': layers,\n",
    "            #    'n_latent': 4, 'activation': 'relu', 'elSizeMax': [1.0,1.0,0.35], 'elSizeMin': [1.0,1.2,0.25], 'variableElSize': True,\n",
    "            #    'n_latent': 6, 'activation': 'relu', 'elSizeMax': [0.19,0.19,0.2], 'elSizeMin': [0.21,0.21,0.25], 'variableElSize': None,\n",
    "               'n_latent': 3, 'activation': 'relu', 'elSizeMax': [0.19,0.19,0.28], 'elSizeMin': [0.21,0.21,0.3], 'variableElSize': None,\n",
    "               'nIE1': 20, 'nIE2': 8, 'kIE': 0.5, 'sizeIE': 0.25, 'typeIE': 'radial',\n",
    "               'nBE': 40, 'kBE': 0.5, 'sizeBE': 1,\n",
    "               'nD1': 0, 'nD2': 0, 'kD': 0.5, 'sizeD': 0.1,\n",
    "               'fd_l_weight': 0.03, 'elliptc_weight': 0}\n",
    "\n",
    "if hyperParams['typeIE'] == 'radial':\n",
    "    nodesIE = GetRadialEncoderInputMask(hyperParams['nIE1'],hyperParams['nIE2'],k=hyperParams['kIE'],Esize=hyperParams['sizeIE'])\n",
    "elif hyperParams['typeIE'] == 'squeare':\n",
    "    nodesIE = GetSquareEncoderInputMask(hyperParams['nIE1'],hyperParams['nIE2'],k=hyperParams['kIE'],Esize=hyperParams['sizeIE'])\n",
    "else:\n",
    "    raise ValueError('typeIE must be either \"radial\" or \"squeare\"')\n",
    "\n",
    "distanceBE = GetBoundaryEncoderInputMask(hyperParams['nBE'],k=hyperParams['kBE'],Esize=hyperParams['sizeBE'])\n",
    "nodesD = GetRadialEncoderInputMask(hyperParams['nD1'],hyperParams['nD2'],k=hyperParams['kD'],Esize=hyperParams['sizeD'])\n",
    "hyperParams['N_interiorEncoder'] = nodesIE.shape[0]\n",
    "hyperParams['N_boundarEncoder'] = distanceBE.shape[0]\n",
    "hyperParams['N_decoderOut'] = nodesD.shape[0]\n",
    "hyperParams['nodesIE'] = nodesIE.tolist()\n",
    "hyperParams['distanceBE'] = distanceBE.tolist()\n",
    "hyperParams['nodesD'] = nodesD.tolist()\n",
    "\n",
    "# dataset = GetInteriorDatasetFixed(data_train[:], nodesIE, nodesD, batch_size=batch_size[0], n_batches=n_batches, elSizeMin=hyperParams['elSizeMin'][0], elSizeMax=hyperParams['elSizeMax'][0], variableElSize=hyperParams['variableElSize'])\n",
    "# dataset_b = GetBoundaryDatasetFixed(data_train[:], nodesIE, distanceBE, nodesD, batch_size=batch_size[1], n_batches=n_batches, elSizeMin=hyperParams['elSizeMin'][1], elSizeMax=hyperParams['elSizeMax'][1], elSizeBF=hyperParams['variableElSize'])\n",
    "# dataset_c = GetCornerDatasetFixed(data_train[:], nodesIE, distanceBE, nodesD, batch_size=batch_size[2], n_batches=n_batches, elSizeMin=hyperParams['elSizeMin'][2], elSizeMax=hyperParams['elSizeMax'][2])\n",
    "\n",
    "# encoder,encoderB,decoder = GetModels(hyperParams)\n",
    "# P = tf.Variable(tf.constant(tf.eye(hyperParams['n_latent']*2),dtype=tf.float32))\n",
    "# r = hyperParams['n_latent']\n",
    "\n",
    "# optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.1e-4)\n",
    "\n",
    "# history = TrainModel(encoder, encoderB, decoder, P, optimizer, dataset, dataset_b, dataset_c, r, n_epochs, n_batches, hyperParams,data_test=data_test_remesh[:10],data_train=data_train_remesh[:10])\n",
    "history = TrainModel(encoder, encoderB, decoder, P, optimizer, dataset, dataset_b, dataset_c, r, n_epochs, n_batches, hyperParams,history=history,data_test=data_test_remesh[:10],data_train=data_train_remesh[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SINNenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
